# AWS Configuration
aws_region = "ap-northeast-1"

# Project Configuration (REQUIRED)
project = "rcs"  # プロジェクト名を指定してください
env     = "prd"  # 環境名を指定してください
app     = "web"  # アプリケーション名を指定してください（カタログ名として使用）

# S3 Configuration (REQUIRED)
# ログが保存されているS3バケット名とプレフィックスを指定してください
# このバケットはAthenaクエリ結果の保存にも使用されます
logs_bucket_name = "rcs-prd-logs-data"                         # S3バケット名（ログ＆Athenaクエリ結果共用）
logs_s3_prefix   = "firelens/firelens/fluent-bit-logs"         # バケット内のプレフィックス
# 最終的なS3パス:
#   ログ: s3://logs_bucket_name/logs_s3_prefix/{log_type}/yyyy/mm/dd/hh/
#   Athenaクエリ結果: s3://logs_bucket_name/athena-query-results/

# Optional S3 設定 (デフォルト値が project-env から自動生成されます)
# 注意: Athenaクエリ結果は logs_bucket_name と同じバケットに保存されます（athena-query-results/プレフィックス下）

# DDL Query Creation Setting
# AthenaクエリエディタにDDLクエリ（CREATE TABLE、CREATE VIEW）を保存するかどうか
create_ddl_queries = false  # false: SELECT文のみ保存（推奨）, true: DDLクエリも保存

# Athena Configuration (Optional - defaults will be generated from project variables)
# athena_database_name = "rcs_prd_web_logs"                     # Default: {project}_{env}_{app}_logs format
# データソース名: {project}-{env} (例: rcs-prd)
# カタログ名: {app} (例: web)
# データベース名: {project}_{env}_{app}_logs (例: rcs_prd_web_logs) - 環境と案件が明確に識別可能

# ===== S3設定の例 =====
#
# 1. Firelens標準パターン:
#    logs_bucket_name = "my-app-logs-data"
#    logs_s3_prefix   = "firelens/firelens/fluent-bit-logs"
#    → ログ: s3://my-app-logs-data/firelens/firelens/fluent-bit-logs/{log_type}/yyyy/mm/dd/hh/
#    → Athenaクエリ結果: s3://my-app-logs-data/athena-query-results/
#
# 2. カスタムパターン1:
#    logs_bucket_name = "production-logs"
#    logs_s3_prefix   = "app-logs/containers"
#    → ログ: s3://production-logs/app-logs/containers/{log_type}/yyyy/mm/dd/hh/
#    → Athenaクエリ結果: s3://production-logs/athena-query-results/
#
# 3. カスタムパターン2:
#    logs_bucket_name = "company-logs-bucket"
#    logs_s3_prefix   = "ecs/microservices"
#    → ログ: s3://company-logs-bucket/ecs/microservices/{log_type}/yyyy/mm/dd/hh/
#    → Athenaクエリ結果: s3://company-logs-bucket/athena-query-results/
#
# 4. シンプルパターン:
#    logs_bucket_name = "simple-logs"
#    logs_s3_prefix   = "logs"
#    → ログ: s3://simple-logs/logs/{log_type}/yyyy/mm/dd/hh/
#    → Athenaクエリ結果: s3://simple-logs/athena-query-results/

# Log Types Configuration
# デフォルト設定では django_web, nginx_web, error に対応しています
# 他のログタイプを追加する場合は以下のように設定してください：
#
# log_types = {
#   django_web = {
#     table_name_suffix = "django_web"
#     description       = "Django web application logs"
#     schema = {
#       date = {
#         type        = "string"
#         description = "Log timestamp"
#       }
#       source = {
#         type        = "string"
#         description = "Log source (stdout/stderr)"
#       }
#       log = {
#         type        = "string"
#         description = "Log message content"
#       }
#       container_id = {
#         type        = "string"
#         description = "Container ID"
#       }
#       container_name = {
#         type        = "string"
#         description = "Container name"
#       }
#       ec2_instance_id = {
#         type        = "string"
#         description = "EC2 instance ID"
#       }
#       ecs_cluster = {
#         type        = "string"
#         description = "ECS cluster name"
#       }
#       ecs_task_arn = {
#         type        = "string"
#         description = "ECS task ARN"
#       }
#       ecs_task_definition = {
#         type        = "string"
#         description = "ECS task definition"
#       }
#     }
#   }
#   nginx_web = {
#     table_name_suffix = "nginx_web"
#     description       = "Nginx web server logs"
#     schema = {
#       # 同様にスキーマを定義
#     }
#   }
#   # 新しいログタイプを追加する場合はここに追加
#   # custom_app = {
#   #   table_name_suffix = "custom_app"
#   #   description       = "Custom application logs"
#   #   schema = {
#   #     # カスタムスキーマを定義
#   #   }
#   # }
# }

# Tags (Optional - default tags will be applied automatically)
# 追加のタグが必要な場合は以下を設定してください:
# tags = {
#   Owner       = "platform-team"
#   CostCenter  = "engineering"
#   Backup      = "daily"
# }

# S3 Bucket Auto-Creation Setting
# S3バケットが存在しない場合の自動作成設定
auto_create_bucket = false  # true: 確認なしで自動作成, false: 手動確認を求める (CI/CD環境では true に設定)

# ===============================
# Glue Crawler 自動実行設定
# ===============================

# Glue Crawlerの自動実行を有効にするかどうか
enable_crawler_schedule = true  # true: スケジュール実行を有効, false: 手動実行のみ

# クローラーの実行スケジュール（cron または rate 形式）
# 新しいログファイルが追加されるタイミングに合わせて設定してください

# 推奨設定例:
crawler_schedule_expression = "cron(0 3 * * ? *)"  # 毎日午前3時に実行

# その他のスケジュール例:
# crawler_schedule_expression = "cron(0 */4 * * ? *)"    # 4時間ごとに実行
# crawler_schedule_expression = "cron(0 6,18 * * ? *)"   # 午前6時と午後6時に実行
# crawler_schedule_expression = "rate(2 hours)"          # 2時間ごとに実行
# crawler_schedule_expression = "rate(30 minutes)"       # 30分ごとに実行

# Crawlerの同時実行数制限（通常は1で十分）
crawler_max_concurrent_runs = 1

# ===============================
# QuickSight 統合設定（オプション）
# ===============================

# QuickSight統合を有効にする場合（追加料金が発生します）
# enable_quicksight = true

# ===============================
# タグ戦略設定（オプション）
# ===============================

# 追加タグを設定する場合:
# tags = {
#   Owner         = "analytics-team"
#   Department    = "engineering"
#   BusinessUnit  = "platform"
#   CostCenter    = "analytics"
#   Monitoring    = "enhanced"
# }

# 運用管理タグの詳細設定:
# owner_team = "analytics-team"
# owner_email = "analytics@yourcompany.com"
# cost_center = "analytics"
# data_classification = "internal"  # public, internal, confidential, restricted
# monitoring_level = "basic"        # basic, enhanced
# schedule = "24x7"                 # 24x7, business-hours
# retention_period = "7-years"      # データ保持期間
